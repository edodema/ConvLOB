{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ohca38Wlrfd"
   },
   "source": [
    "# Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52YZv1UwmjZR",
    "outputId": "1f8033e2-c826-4496-feea-e4f1cf5a263f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd /content/gdrive/MyDrive/Colab Notebooks/AI4T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJDVJSHcOWuW"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f86XG8mzlgeN",
    "outputId": "b92fe9ba-20ea-4290-8dbb-d58d9ff2ecf5"
   },
   "outputs": [],
   "source": [
    "%pip install pytorch_lightning neptune-client -qqq\n",
    "%pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PxwTBVDOehc"
   },
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cJDPcXiOM5R"
   },
   "source": [
    "# Dataset\n",
    "Object used to load data, an input level is defined by a tuple $(p_a, v_a, p_b, v_b)$, standing to price and volume for both *ask* and *bid*.\n",
    "This time instead of having 40 values we unflatten the levels ending up with a 4D channel, we make more explicit that we have 10 levels of 4 tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7b23yttlgeP"
   },
   "outputs": [],
   "source": [
    "class LOBDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: np.ndarray,\n",
    "        input_idx: int = 40,\n",
    "        label_idx: int = -5,\n",
    "        window: int = 100,\n",
    "        pred_horizon_idx: int = -1,\n",
    "    ):\n",
    "        \"\"\"Dataset object for FI-2010 dataset.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): Input data array.\n",
    "            input_idx (int, optional): Last column for input data. Defaults to 40.\n",
    "            label_idx (int, optional): First column for labels. Defaults to -5.\n",
    "            window (int, optional): Window size. Defaults to 100.\n",
    "            pred_horizon_idx (int, optional): Prediction horizon index. Defaults to -1.\n",
    "        \"\"\"\n",
    "        super(LOBDataset, self).__init__()\n",
    "        x, y = self._init_data(\n",
    "            data=data,\n",
    "            in_idx=input_idx,\n",
    "            gt_idx=label_idx,\n",
    "            win=window,\n",
    "            ph_idx=pred_horizon_idx,\n",
    "        )\n",
    "\n",
    "        self.x = torch.from_numpy(x.copy())\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def _init_data(\n",
    "        self, data: np.ndarray, in_idx: int, gt_idx: int, win: int, ph_idx: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Preprocess data.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): Input data.\n",
    "            in_idx (int): Last column of input data we aim to consider.\n",
    "            gt_idx (int): First column of ground truth we aim to consider.\n",
    "            win (int): Window size.\n",
    "            ph_idx (int): Prediction horizon index.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Preprocessed input and ground truth data.\n",
    "        \"\"\"\n",
    "        # Input data are the first `in_idx`` (40 by default) columns i.e. the first 10 levels\n",
    "        # due to each level being defined by a 4-tuple (price_bid, volume_bid, price_ask, volume_ask).\n",
    "        x = data[:, :in_idx]\n",
    "\n",
    "        # Labels are the last `gt_idx`` (5 by default) columns of the LOB. Possible values are:\n",
    "        # - 1: Positive percentage change.\n",
    "        # - 2: Stationary behavior.\n",
    "        # - 3: Negative percentage change.\n",
    "        # We also want them to start from 0.\n",
    "        y = data[:, -gt_idx:] - 1\n",
    "\n",
    "        # Each of the `gt_idx` columns represents a different projection horizon, for simplicity we keep one only.\n",
    "        y = y[:, ph_idx]\n",
    "\n",
    "        # We split the input data in windows of length `win`, then trim the first `win` elements of the labels.\n",
    "        x_win, y_trim = self._slide_window(x=x, y=y, win=win)\n",
    "\n",
    "        return x_win, y_trim\n",
    "\n",
    "    def _slide_window(\n",
    "        self, x: np.ndarray, y: np.ndarray, win: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Split data in windows.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input data.\n",
    "            y (np.ndarray): Ground truth.\n",
    "            win (int): Window size.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Obtained windows with their ground truth.\n",
    "        \"\"\"\n",
    "        x_win = sliding_window_view(x=x, window_shape=win, axis=0).transpose(0, 2, 1)\n",
    "        y_trim = y[win - 1 :]\n",
    "        return x_win, y_trim\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Data length.\n",
    "\n",
    "        Returns:\n",
    "            int: Length.\n",
    "        \"\"\"\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, item: int) -> List[torch.Tensor]:\n",
    "        \"\"\"Get item by index.\n",
    "\n",
    "        Args:\n",
    "            item (int): Index.\n",
    "\n",
    "        Returns:\n",
    "            List[torch.Tensor]: List with input data and label corresponding to the specified index.\n",
    "        \"\"\"\n",
    "        return [self.x[item], self.y[item]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use *PyTorch Lightning*, this allows us to have a cleaner code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRDwtQCclgeQ"
   },
   "outputs": [],
   "source": [
    "class LOBDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path,\n",
    "        batch_size: int,\n",
    "        input_idx: int = 40,\n",
    "        label_idx: int = -5,\n",
    "        window: int = 100,\n",
    "        pred_horizon_idx: int = -1,\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.input_idx = input_idx\n",
    "        self.label_idx = label_idx\n",
    "        self.window = window\n",
    "        self.pred_horizon_idx = pred_horizon_idx\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare data.\"\"\"\n",
    "        #data_file = self.data_dir / \"data.zip\"\n",
    "        data_file = \"drive/shared-with-me\n",
    "\n",
    "        # * Download data.zip if necessary.\n",
    "        if not data_file.exists():\n",
    "            url = \"https://raw.githubusercontent.com/zcakhaa/DeepLOB-Deep-Convolutional-Neural-Networks-for-Limit-Order-Books/master/data/data.zip\"\n",
    "\n",
    "            # Download.\n",
    "            print(f\"Downloading data from {url}...\")\n",
    "            r = requests.get(url)\n",
    "            open(data_file, \"wb\").write(r.content)\n",
    "\n",
    "            # Extract.\n",
    "            print(f\"Inflating {data_file}...\")\n",
    "            with zipfile.ZipFile(data_file, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(self.data_dir)\n",
    "\n",
    "        # # * Data preprocessing.\n",
    "        train_path = self.data_dir / \"train.gz\"\n",
    "        val_path = self.data_dir / \"val.gz\"\n",
    "        test_path = self.data_dir / \"test.gz\"\n",
    "\n",
    "        # If data is not preprocessed, do it.\n",
    "        if not all(f.exists() for f in [train_path, val_path, test_path]):\n",
    "            self._prepare_data()\n",
    "            print()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Load and split data according to a 80-20 rate, then save the new splits.\n",
    "\n",
    "        Args:\n",
    "            path (Path): Path of the FI-2010 `data.zip` file.\n",
    "        \"\"\"\n",
    "\n",
    "        train_file = \"Train_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "        test_files = [\n",
    "            \"Test_Dst_NoAuction_DecPre_CF_7.txt\",\n",
    "            \"Test_Dst_NoAuction_DecPre_CF_8.txt\",\n",
    "            \"Test_Dst_NoAuction_DecPre_CF_9.txt\",\n",
    "        ]\n",
    "\n",
    "        # * Prepare data.\n",
    "        # Load as NumPy arrays.\n",
    "\n",
    "        print(f\"Loading {train_file}...\")\n",
    "        train_val = np.loadtxt(self.data_dir / train_file)\n",
    "\n",
    "        # Split into train and val according to a 80-20 ratio.\n",
    "        train = train_val[:, : int(np.floor(train_val.shape[1] * 0.8))]\n",
    "        val = train_val[:, int(np.floor(train_val.shape[1] * 0.8)) :]\n",
    "\n",
    "        test = []\n",
    "        for f in test_files:\n",
    "            print(f\"Loading {train_file}...\")\n",
    "            test.append(np.loadtxt(self.data_dir / f))\n",
    "\n",
    "        test = np.hstack(test)\n",
    "\n",
    "        # * Save data.\n",
    "        print(f\"Saving {self.data_dir / 'train.gz'}...\")\n",
    "        np.savetxt(self.data_dir / \"train.gz\", train.T)\n",
    "\n",
    "        print(f\"Saving {self.data_dir / 'val.gz'}...\")\n",
    "        np.savetxt(self.data_dir / \"val.gz\", val.T)\n",
    "\n",
    "        print(f\"Saving {self.data_dir / 'test.gz'}...\")\n",
    "        np.savetxt(self.data_dir / \"test.gz\", test.T)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Setup datasets.\n",
    "\n",
    "        Args:\n",
    "            stage (Optional[str], optional): Stage in which we are e.g. \"fit\", \"test\". Defaults to None.\n",
    "        \"\"\"\n",
    "        # Assign train/val splits.\n",
    "        if stage in (None, \"fit\"):\n",
    "            train_file = self.data_dir / \"train.gz\"\n",
    "            print(f\"Loading {train_file}...\")\n",
    "            self.train = LOBDataset(\n",
    "                data=np.loadtxt(train_file),\n",
    "                input_idx=self.input_idx,\n",
    "                label_idx=self.label_idx,\n",
    "                window=self.window,\n",
    "                pred_horizon_idx=self.pred_horizon_idx,\n",
    "            )\n",
    "\n",
    "            val_file = self.data_dir / \"val.gz\"\n",
    "            print(f\"Loading {val_file}...\")\n",
    "            self.val = LOBDataset(\n",
    "                data=np.loadtxt(val_file),\n",
    "                input_idx=self.input_idx,\n",
    "                label_idx=self.label_idx,\n",
    "                window=self.window,\n",
    "                pred_horizon_idx=self.pred_horizon_idx,\n",
    "            )\n",
    "        # Assign test split.\n",
    "        if stage in (None, \"test\"):\n",
    "            test_file = self.data_dir / \"test.gz\"\n",
    "            print(f\"Loading {test_file}...\")\n",
    "            self.test = LOBDataset(\n",
    "                data=np.loadtxt(test_file),\n",
    "                input_idx=self.input_idx,\n",
    "                label_idx=self.label_idx,\n",
    "                window=self.window,\n",
    "                pred_horizon_idx=self.pred_horizon_idx,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get train dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: Train dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get val dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: Val dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get test dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: Test dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gz2coGHblgeS"
   },
   "source": [
    "# Model\n",
    "This model is defined by a series of 2D convolution, this time there is no need to add one dimension for channels.\n",
    "In this model we aim to exploit sequential informations through attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkRfSP7BlgeU"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Flatten module.\"\"\"\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, dim: int = -1):\n",
    "        \"\"\"Squeeze module.\n",
    "\n",
    "        Args:\n",
    "            dim (int, optional): Dimension on which we squeeze. Defaults to -1.\n",
    "        \"\"\"\n",
    "        super(Squeeze, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return x.squeeze(dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unflatten(nn.Module):\n",
    "    def __init__(self, channels: int = 4):\n",
    "        \"\"\"Unflatten module.\n",
    "\n",
    "        Args:\n",
    "            channels (int, optional): Number of channels we want. Defaults to 4.\n",
    "        \"\"\"\n",
    "        super(Unflatten, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        b, h, w = x.shape\n",
    "        return x.view(b, self.channels, h, w // self.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 4,\n",
    "        out_channels: int = 4,\n",
    "        kernel: int = 3,\n",
    "        stride: int = 1,\n",
    "    ):\n",
    "        \"\"\"Basic 2D convolutional block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int, optional): Input channels. Defaults to 4.\n",
    "            out_channels (int, optional): Output channels. Defaults to 4.\n",
    "            kernel (int, optional): Kernel size. Defaults to 3.\n",
    "            stride (int, optional): Stride dimension. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super(Conv2D, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel,\n",
    "                stride=stride,\n",
    "                padding=0,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 4,\n",
    "        out_channels: int = 4,\n",
    "        kernel: int = 3,\n",
    "        stride: int = 1,\n",
    "    ):\n",
    "        \"\"\"Basic 1D convolutional block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int, optional): Input channels. Defaults to 4.\n",
    "            out_channels (int, optional): Output channels. Defaults to 4.\n",
    "            kernel (int, optional): Kernel size. Defaults to 3.\n",
    "            stride (int, optional): Stride dimension. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super(Conv1D, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel,\n",
    "                stride=stride,\n",
    "                padding=0,\n",
    "            ),\n",
    "            nn.BatchNorm1d(num_features=out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, embed_dim: int, num_heads: int\n",
    "    ):\n",
    "        \"\"\"Basic attention module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Input channels.\n",
    "            out_channels (int): Output channels.\n",
    "            embed_dim (int): Embedding dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        # Convolutions for query, key and value.\n",
    "        self.q = Conv1D(\n",
    "            in_channels=in_channels, out_channels=embed_dim, kernel=3, stride=1\n",
    "        )\n",
    "        self.k = Conv1D(\n",
    "            in_channels=in_channels, out_channels=embed_dim, kernel=3, stride=1\n",
    "        )\n",
    "        self.v = Conv1D(\n",
    "            in_channels=in_channels, out_channels=embed_dim, kernel=3, stride=1\n",
    "        )\n",
    "\n",
    "        # Multihead attention.\n",
    "        self.att = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads, dropout=0\n",
    "        )\n",
    "\n",
    "        self.conv = Conv1D(\n",
    "            in_channels=embed_dim, out_channels=out_channels, kernel=1, stride=2\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        q = self.q(x).permute(2, 0, 1)\n",
    "        k = self.k(x).permute(2, 0, 1)\n",
    "        v = self.v(x).permute(2, 0, 1)\n",
    "\n",
    "        out = self.att(query=q, key=k, value=v, need_weights=False)[0].permute(1, 2, 0)\n",
    "        out = self.conv(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes: int = 3):\n",
    "        \"\"\"Neural network.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int, optional): Number of classes. Defaults to 3.\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # Get (pb,vb,pa,ba) as channels.\n",
    "            Unflatten(channels=4),\n",
    "            # Feature extraction.\n",
    "            Conv2D(in_channels=4, out_channels=16, kernel=3, stride=2),\n",
    "            Conv2D(in_channels=16, out_channels=8, kernel=3, stride=1),\n",
    "            Conv2D(in_channels=8, out_channels=64, kernel=1, stride=2),\n",
    "            # We now work on three dimensional data.\n",
    "            Squeeze(dim=-1),\n",
    "            Attention(in_channels=64, out_channels=16, embed_dim=32, num_heads=8),\n",
    "            # We flatten everything.\n",
    "            Flatten(),\n",
    "            nn.Linear(in_features=176, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits.\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PyTorch Lightning* allows us to define basic training, validation and test steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVk4LmcqlgeV"
   },
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float,\n",
    "        decay: bool = False,\n",
    "        num_classes: int = 3,\n",
    "    ):\n",
    "        \"\"\"Neural network Lightning module.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int, optional): Number of classes. Defaults to 3.\n",
    "        \"\"\"\n",
    "        super(LitModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = Model()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # * Metrics.\n",
    "        self.train_recall = torchmetrics.Recall()\n",
    "        self.val_recall = torchmetrics.Recall()\n",
    "        self.test_recall = torchmetrics.Recall()\n",
    "\n",
    "        self.train_precision = torchmetrics.Precision()\n",
    "        self.val_precision = torchmetrics.Precision()\n",
    "        self.test_precision = torchmetrics.Precision()\n",
    "\n",
    "        self.train_f1 = torchmetrics.F1Score()\n",
    "        self.val_f1 = torchmetrics.F1Score()\n",
    "        self.test_f1 = torchmetrics.F1Score()\n",
    "\n",
    "        self.train_cohen = torchmetrics.CohenKappa(num_classes=num_classes)\n",
    "        self.val_cohen = torchmetrics.CohenKappa(num_classes=num_classes)\n",
    "        self.test_cohen = torchmetrics.CohenKappa(num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def step(self, batch: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "        \"\"\"Base step.\n",
    "\n",
    "        Args:\n",
    "            batch (List[torch.Tensor]): Input batch.\n",
    "\n",
    "        Returns:\n",
    "            List[torch.Tensor]: Loss and predictions' indices.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        x = x.to(torch.float)\n",
    "        y = y.to(torch.long)\n",
    "\n",
    "        logits = self(x)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        return [loss, preds]\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: List[torch.Tensor], batch_idx: Optional[int]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Base trining step.\n",
    "\n",
    "        Args:\n",
    "            batch (List[torch.Tensor]): Input batch.\n",
    "            batch_idx (Optional[int]): Input batch's index.\n",
    "\n",
    "        Returns:\n",
    "            [torch.Tensor]: Loss.\n",
    "        \"\"\"\n",
    "        y = batch[1].to(torch.long)\n",
    "        loss, preds = self.step(batch)\n",
    "\n",
    "        self.train_recall(preds, y)\n",
    "        self.train_precision(preds, y)\n",
    "        self.train_f1(preds, y)\n",
    "        self.train_cohen(preds, y)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train/loss\": loss,\n",
    "                \"train/recall\": self.train_recall.compute(),\n",
    "                \"train/precision\": self.train_precision.compute(),\n",
    "                \"train/f1\": self.train_f1.compute(),\n",
    "                \"train/cohen\": self.train_cohen.compute(),\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: List[torch.Tensor], batch_idx: Optional[int]):\n",
    "        \"\"\"Base validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (List[torch.Tensor]): Input batch.\n",
    "            batch_idx (Optional[int]): Input batch's index.\n",
    "        \"\"\"\n",
    "        y = batch[1].to(torch.long)\n",
    "        loss, preds = self.step(batch)\n",
    "\n",
    "        self.val_recall(preds, y)\n",
    "        self.val_precision(preds, y)\n",
    "        self.val_f1(preds, y)\n",
    "        self.val_cohen(preds, y)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"val/loss\": loss,\n",
    "                \"val/recall\": self.val_recall.compute(),\n",
    "                \"val/precision\": self.val_precision.compute(),\n",
    "                \"val/f1\": self.val_f1.compute(),\n",
    "                \"val/cohen\": self.val_cohen.compute(),\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch: List[torch.Tensor], batch_idx: Optional[int]):\n",
    "        \"\"\"Base test step.\n",
    "\n",
    "        Args:\n",
    "            batch (List[torch.Tensor]): Input batch.\n",
    "            batch_idx (Optional[int]): Input batch's index.\n",
    "        \"\"\"\n",
    "        y = batch[1].to(torch.long)\n",
    "        loss, preds = self.step(batch)\n",
    "\n",
    "        self.test_recall(preds, y)\n",
    "        self.test_precision(preds, y)\n",
    "        self.test_f1(preds, y)\n",
    "        self.test_cohen(preds, y)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"test/loss\": loss,\n",
    "                \"test/recall\": self.test_recall.compute(),\n",
    "                \"test/precision\": self.test_precision.compute(),\n",
    "                \"test/f1\": self.test_f1.compute(),\n",
    "                \"test/cohen\": self.test_cohen.compute(),\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> List[torch.optim.Optimizer]:\n",
    "        \"\"\"Configure optimizer.\n",
    "\n",
    "        Returns:\n",
    "            List[torch.optim.Optimizer]: List of optimizers.\n",
    "        \"\"\"\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "        if not self.hparams.decay:\n",
    "            return [opt]\n",
    "        else:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt)\n",
    "            return [opt, scheduler]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CipUimfBlgeW"
   },
   "source": [
    "# Training\n",
    "Here we define the actual training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y6QB5oXUlgeW",
    "outputId": "b66a497f-04f9-414f-be45-bf0ee232dc2e"
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "ROOT_DIR = Path(\".\")\n",
    "\n",
    "datamodule = LOBDataModule(data_dir=ROOT_DIR / \"data\", batch_size=32)\n",
    "model = LitModel(lr=1e-3, decay=False)\n",
    "\n",
    "logger = NeptuneLogger(\n",
    "    project=\"user/project\",\n",
    "    api_key=\"key\"\n",
    "    tags=[\"Att\"],\n",
    ")\n",
    "\n",
    "# ! Need to choose for which metrics we want to monitor.\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=-1 if torch.cuda.is_available() else 0,\n",
    "    max_epochs=100,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LOB-T.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
